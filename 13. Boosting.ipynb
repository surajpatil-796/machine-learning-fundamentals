{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "##### What is Boosting?\n",
    "Definition: The term ‘Boosting’ refers to a family of algorithms which converts weak learner to strong learners.\n",
    "\n",
    "##### How Boosting Algorithms works?\n",
    "    \n",
    "    Step 1:  The base learner takes all the distributions and assign equal weight or attention to each observation.\n",
    "\n",
    "    Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\n",
    "\n",
    "    Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n",
    "\n",
    "    Finally, it combines the outputs from weak learner and creates  a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules.\n",
    "    \n",
    "##### Types of Boosting Algorithms\n",
    "Underlying engine used for boosting algorithms can be anything.  It can be decision stamp, margin-maximizing classification algorithm etc. There are many boosting algorithms which use other types of engine such as:\n",
    "\n",
    "    AdaBoost (Adaptive Boosting)\n",
    "    Gradient Tree Boosting\n",
    "    XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Adaptive Boosting aka AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading of the data\n",
    "data = pd.read_csv(\"bank.csv\")\n",
    "\n",
    "X = data.drop('deposit', axis=1)\n",
    "y = data['deposit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into test and train\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, random_state = 0,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision Tree score:  0.7094655120931621\n"
     ]
    }
   ],
   "source": [
    "#Fitting of Weak Classifier\n",
    "# Weak Learner or weak classifier means the algorithm that consists of error or mistakes\n",
    "dt_clf = DecisionTreeClassifier(max_depth=1,random_state=0)\n",
    "dt_clf.fit(X_train,y_train)\n",
    "dt_score = dt_clf.score(X_test,y_test)\n",
    "print(\"decision Tree score: \",dt_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ada Boosting:\n",
    "    1- Preparation of weak classifier - like above cell Decision Tree\n",
    "    2- Calculation of misclassification rate (how many data points are wrongly classsified a true or false)\n",
    "    3- Calculation of 'stage value' (stage value is used to weight predictions from the model based on the misclassification error for the model.)\n",
    "    4- Updation of training weights (the training weights are updated giving more weight to incorrectly predicted instances, and less weight to correctly predicted instances.)\n",
    "\n",
    "Note: The initial weight before applying adaboost is set to 1/n (Where n is the total number of instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=1,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort=False,\n",
       "                                                         random_state=0,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=1.0, n_estimators=50, random_state=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting of weak classifier with Adaboost\n",
    "ada_clf = AdaBoostClassifier(base_estimator=dt_clf,random_state=0)\n",
    "ada_clf.fit(X_train,y_train)\n",
    "# n_estimators: It controls the number of weak learners.\n",
    "# learning_rate:Controls the contribution of weak learners in the final combination. There is a trade-off between learning_rate and n_estimators.\n",
    "# base_estimators: It helps to specify different ML algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada boost Score:  0.8244252015527023\n"
     ]
    }
   ],
   "source": [
    "ada_score = ada_clf.score(X_test,y_test)\n",
    "print(\"Ada boost Score: \",ada_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Tree Boosting\n",
    "Gradient boosting approaches the problem a bit differently. Instead of adjusting weights of data points, Gradient boosting focuses on the difference between the prediction and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=0, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#Default model is Decision Tree\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "gb_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Score:  0.8462227530606151\n"
     ]
    }
   ],
   "source": [
    "gb_score = gb_clf.score(X_test,y_test)\n",
    "print(\"Gradient Boost Score: \",gb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost\n",
    "XGBoost (standing for eXtreme Gradient Boosting) \n",
    "\n",
    "Gradient boosting machines are generally very slow in implementation because of sequential model training. Hence, they are not very scalable\n",
    "\n",
    "However, it all changed with XGboost Library. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.\n",
    "\n",
    "XGBoost has better because:\n",
    "1. Handling sparse data: Missing values or data processing steps like one-hot encoding make data sparse. XGBoost incorporates a sparsity-aware split finding algorithm to handle different types of sparsity patterns in the data.\n",
    "2. Regularization: XGBoost has an option to penalize complex models using both L1 and L2 regularization. Regularization helps prevent overfitting.\n",
    "3. Weighted quantile sketch: XGBoost has a distributed weighted quantile sketch algorithm to effectively handle weighted data.\n",
    "4. Out-of-core computing: This feature optimizes the available disk space and maximizes its usage when handling huge datasets that do not fit into memory\n",
    "5. Block structure for parallel learning: For faster computing, XGBoost can make use of multiple cores on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=1,\n",
       "                                                    max_features=None,\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    presort=False,\n",
       "                                                    random_state=0,\n",
       "                                                    splitter='best'),\n",
       "              base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              c...\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "              validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_clf = XGBClassifier(base_estimator=dt_clf, random_state=0)\n",
    "xgb_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Score:  0.8504031054045984\n"
     ]
    }
   ],
   "source": [
    "xgb_score = xgb_clf.score(X_test,y_test)\n",
    "print(\"XGBoost Score: \",xgb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
